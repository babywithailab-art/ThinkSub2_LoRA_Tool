== ThinkSub2_LoRA_Tool ==

ğŸ™ Personal Whisper Model Builder
(LoRA â†’ large-v3-turbo ë³‘í•© â†’ faster-whisper ëª¨ë¸ ìë™ ìƒì„±)
ğŸ“¢ ì½ì–´ì£¼ì„¸ìš”

ì´ í”„ë¡œì íŠ¸ëŠ” íŠ¹ì • ì¸ë¬¼(ê°œì¸ í™”ì)ì˜ ìŒì„±ì„ ë” ì •í™•í•˜ê²Œ ì¸ì‹í•˜ê¸° ìœ„í•´
LoRA í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Whisper large-v3-turbo ëª¨ë¸ì„ ìë™ íŠœë‹í•˜ê³ ,
faster-whisperì—ì„œ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ ìƒì„±í•´ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.

ì‚¬ìš©ìê°€ ëˆ„ì í•œ ìŒì„± ë°ì´í„°(wav + text)ë¥¼ ì´ìš©í•´:

LoRA ê¸°ë°˜ íŒŒì¸íŠœë‹ ìˆ˜í–‰

large-v3-turbo ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©

faster-whisper(CTranslate2) í¬ë§·ìœ¼ë¡œ ë³€í™˜

ê¹Œì§€ì˜ ê³¼ì •ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬
â€œê°œì¸ ë§ì¶¤ STT ëª¨ë¸â€ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.

âœ¨ ì£¼ìš” ê¸°ëŠ¥

âœ… íŠ¹ì • í™”ì(ê°œì¸) ìŒì„± ì¸ì‹ë¥  í–¥ìƒ
âœ… LoRA í•™ìŠµ â†’ ëª¨ë¸ ë³‘í•© â†’ CT2 ë³€í™˜ ìë™ ì²˜ë¦¬
âœ… manifest.jsonl ë˜ëŠ” ë°ì´í„° í´ë” ì„ íƒë§Œìœ¼ë¡œ ì‹¤í–‰
âœ… faster-whisperì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì¶œë ¥
âœ… ë°ì´í„°ê°€ ìŒ“ì¼ìˆ˜ë¡ ê°œì¸ ë§ì¶¤ ì„±ëŠ¥ í–¥ìƒ

ğŸ§© ì‚¬ìš© íë¦„
ìŒì„± + ìë§‰ ë°ì´í„° ëˆ„ì 
        â†“
LoRA í•™ìŠµ ìˆ˜í–‰
        â†“
large-v3-turbo ëª¨ë¸ ë³‘í•©
        â†“
CT2 ëª¨ë¸ ë³€í™˜
        â†“
faster-whisperì—ì„œ ì‚¬ìš©


ê²°ê³¼ì ìœ¼ë¡œ,
ë‚´ ëª©ì†Œë¦¬ / ë‚´ ë°œìŒ / ë‚´ í™˜ê²½ì— ë§ì¶˜ ìŒì„± ì¸ì‹ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

âš™ LoRA í•™ìŠµ ì„¤ì • ê°€ì´ë“œ

LoRA íŒŒì¸íŠœë‹ ì‹œ ì•„ë˜ ê°’ë“¤ì„ ì¡°ì •í•˜ì—¬ ëª¨ë¸ ë³€ê²½ ê°•ë„ì™€ í•™ìŠµ ì•ˆì •ì„±ì„ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ”§ LoRA Rank (r)

LoRAê°€ ì›ë³¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ í¬ê²Œ ìˆ˜ì •í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ê°’ì…ë‹ˆë‹¤.

ê°’	ì„¤ëª…
r = 8	ëª¨ë¸ ë³€ê²½ì´ ì‘ì•„ ì•ˆì •ì ì´ì§€ë§Œ ì ì‘ ì†ë„ê°€ ëŠë¦½ë‹ˆë‹¤.
r = 16	ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê· í˜• ì¡íŒ ì„¤ì •ì…ë‹ˆë‹¤.
r = 32	ëª¨ë¸ì„ ê°•í•˜ê²Œ ë³€ê²½í•˜ì—¬ íŠ¹ì • í™”ìì— ë¹ ë¥´ê²Œ ì ì‘í•˜ì§€ë§Œ ê³¼ì í•© ìœ„í—˜ì´ ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

âœ… ê°œì¸ ë§ì¶¤ STTì—ëŠ” ë³´í†µ 16~32 ë²”ìœ„ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.

ğŸŒ§ LoRA Dropout (lora_dropout)

í•™ìŠµ ì‹œ ì¼ë¶€ ì—°ê²°ì„ ëœë¤í•˜ê²Œ ì œì™¸í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

ê°’	ì„¤ëª…
0.05	ê¸°ë³¸ê°’, ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ ì•ˆì •ì ì…ë‹ˆë‹¤.
0.1	ë°ì´í„°ê°€ ì ì„ ë•Œ ê³¼ì í•©ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

ë°ì´í„°ê°€ ì ì€ ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ì—ì„œëŠ” 0.1ì´ ë” ì•ˆì •ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ” í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (epochs)

ëª¨ë¸ì´ ì „ì²´ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ë°ì´í„° ì–‘ì— ë”°ë¼ ì ì ˆí•œ ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

ë°ì´í„° í¬ê¸°	ì¶”ì²œ epochs
â‰¤ 300 ìƒ˜í”Œ	6 ~ 10
300 ~ 1000	4 ~ 6
â‰¥ 1000	2 ~ 4

ë°ì´í„°ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ epochsë¥¼ ì¤„ì´ëŠ” ë°©ì‹ì´ ì•ˆì •ì ì´ë©°,
ë„ˆë¬´ ë§ì´ ë°˜ë³µ í•™ìŠµí•˜ë©´ íŠ¹ì • ë¬¸ì¥ì„ ì™¸ì›Œë²„ë ¤ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¯ ì´ í”„ë¡œì íŠ¸ê°€ í•„ìš”í•œ ê²½ìš°

ê°œì¸ ë°©ì†¡/ì˜ìƒ í¸ì§‘ì—ì„œ ìë§‰ ì •í™•ë„ë¥¼ ë†’ì´ê³  ì‹¶ì€ ê²½ìš°

ë°˜ë³µì ìœ¼ë¡œ ê°™ì€ í™”ìì˜ ìŒì„±ì„ ì¸ì‹í•˜ëŠ” ì‘ì—…

íŠ¹ì • ë°œìŒ/ë§íˆ¬ë¥¼ ë” ì˜ ì¸ì‹ì‹œí‚¤ê³  ì‹¶ì€ ê²½ìš°

Whisper ê¸°ë°˜ STT ì •í™•ë„ë¥¼ ê°œì¸ í™˜ê²½ì— ë§ê²Œ ê°œì„ í•˜ê³  ì‹¶ì€ ê²½ìš°

ğŸš€ ëª©í‘œ

ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ”
Whisper ê¸°ë°˜ ìŒì„± ì¸ì‹ì„ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ê°œì¸í™”í•˜ê³ ,
ìì‹ ì˜ ì‘ì—… í™˜ê²½ì— ë§ê²Œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.



== ThinkSub2_LoRA_Tool ==

ğŸ™ Personal Whisper Model Builder

(LoRA â†’ large-v3-turbo merge â†’ faster-whisper model auto generation)

ğŸ“¢ Please Read

This project helps improve speech recognition accuracy for a specific person (personal speaker) by automatically tuning a Whisper large-v3-turbo model using LoRA training data and generating a model that can be used directly with faster-whisper.

Using accumulated speech data (wav + text), the tool automatically performs:

LoRA-based fine-tuning

Merging with the large-v3-turbo base model

Conversion to faster-whisper (CTranslate2) format

This pipeline makes it easy to build a personalized STT model tailored to a specific speaker.

âœ¨ Main Features

âœ… Improves speech recognition accuracy for a specific speaker
âœ… Automatic LoRA training â†’ model merging â†’ CT2 conversion
âœ… Run by simply selecting a manifest.jsonl file or dataset folder
âœ… Outputs models directly usable in faster-whisper
âœ… Recognition improves as more personal data is accumulated

ğŸ§© Workflow
Accumulate speech + subtitle data
                â†“
          LoRA training
                â†“
 Merge with large-v3-turbo model
                â†“
        Convert to CT2 model
                â†“
      Use in faster-whisper


As a result, you can build a speech recognition model optimized for:

Your voice

Your pronunciation

Your recording environment

âš™ LoRA Training Configuration Guide

When performing LoRA fine-tuning, you can adjust the following values to control how strongly the model adapts and how stable training remains.

ğŸ”§ LoRA Rank (r)

This value determines how strongly LoRA modifies the original model weights.

Value	Description
r = 8	Small model change; stable but adapts slowly
r = 16	Balanced setting commonly used
r = 32	Strong adaptation for a specific speaker, but higher risk of overfitting

âœ… For personal STT models, values between 16 and 32 are generally recommended.

ğŸŒ§ LoRA Dropout (lora_dropout)

Randomly drops connections during training to reduce overfitting.

Value	Description
0.05	Default, stable when enough data is available
0.1	Helps prevent overfitting when data is limited

When starting with a small dataset, 0.1 is often safer.

ğŸ” Training Epochs

Defines how many times the entire dataset is repeated during training.

Choosing epochs based on dataset size is important.

Dataset Size	Recommended Epochs
â‰¤ 300 samples	6 ~ 10
300 ~ 1000	4 ~ 6
â‰¥ 1000	2 ~ 4

As the dataset grows, reducing epochs helps prevent overfitting.
Too many epochs can cause the model to memorize sentences rather than generalize.

ğŸ¯ When This Project Is Useful

Improving subtitle accuracy for personal streaming or video editing

Repeated recognition of the same speaker

Enhancing recognition of specific pronunciations or speech patterns

Personalizing Whisper-based STT for a specific recording environment

ğŸš€ Goal

The goal of this project is to make Whisper-based speech recognition easily personalizable, allowing users to continuously improve recognition performance tailored to their own environment and workflow.
